<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly">
    <meta name="keywords"
        content="Robotic Manipulation, Structural Representation, Model-based Planning, Foundation Models">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ARCH | Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly</title>


    <!-- Thumbnail for social media sharing -->
    <!-- <meta property="og:image" content="media/thumbnail.jpg"> -->

    <!-- Favicon -->
    <!-- <link rel="icon" href="media/thumbnail.jpg" type="image/jpeg"> -->

    <script>
        window.dataLayer = window.dataLayer || [];
    </script>

    <script>
        function updateInTheWild() {
            var task = document.getElementById("inthewild-video-menu").value;

            console.log("updateInTheWild", task)

            var video = document.getElementById("inthewild-video");
            video.src = "media/videos/" +
                task +
                ".m4v"
            video.play();
        }

        function updateBimanual() {
            var task = document.getElementById("bimanual-video-menu").value;

            console.log("updateBimanual", task)

            var video = document.getElementById("bimanual-video");
            video.src = "media/videos/1_" +
                task +
                ".mp4"
            video.play();
        }

        function updateClothes() {
            var task = document.getElementById("clothes-video-menu").value;

            console.log("updateclothes", task)

            var img = document.getElementById("clothes-img");
            img.src = "media/fold-strategies/" +
                task +
                ".jpeg"

            var video = document.getElementById("clothes-video");
            video.src = "media/videos/fold-" +
                task +
                ".mp4"
            video.play();
        }
    </script>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <link rel="stylesheet" href="./static/source_serif_4.css">
    <link rel="stylesheet" href="./static/source_sans_3.css">
    <link rel="stylesheet" href="./static/academicons.min.css">
    <link rel="stylesheet" href="./static/fontawesome/css/fontawesome.css">
    <link rel="stylesheet" href="./static/fontawesome/css/brands.css">
    <link rel="stylesheet" href="./static/fontawesome/css/light.css">


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body onload="updateInTheWild();updateBimanual();">


    <section class="hero">
        <div class="hero-body">
            <div class="container is-fullhd">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">ARCH: Hierarchical Hybrid Learning for Long-Horizon
                            Contact-Rich Robotic Assembly</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://web.stanford.edu/~jksun">Jiankai Sun</a><sup>1</sup>,

                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://aidanreececurtis.com/">Aidan Curtis</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://qq456cvb.github.io/">Yang You</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://decayale.github.io/">Yan Xu</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank"
                                    href="https://www.research.autodesk.com/people/michael-koehle/">Michael
                                    Koehle</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank"
                                    href="https://scholar.google.com/citations?user=5JlEyTAAAAAJ&hl=en">Leonidas
                                    Guibas</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank"
                                    href="https://scholar.google.com/citations?user=S2vB4tAAAAAJ&hl=en">Sachin
                                    Chitta</a><sup>4</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://web.stanford.edu/~schwager/">Mac
                                    Schwager</a><sup>1</sup>,
                            </span>
                            <span class="author-block">
                                <a target="_blank" href="https://www.research.autodesk.com/people/hui-li/">Hui
                                    Li</a><sup>4</sup>
                            </span>
                        </div>
                        <div class="is-size-5 affiliation">
                            <sup>1</sup>Stanford University,
                            <sup>2</sup>MIT,
                            <sup>3</sup>University of Michigan,
                            <sup>4</sup>Autodesk Research
                        </div>
                        <br>
                        <!-- <div class="affiliation-note">
                            <sup>*</sup> indicates equal contributions
                        </div> -->
                        <div class="button-container">
                            <!-- <a href="./ARCH.pdf" target="_blank" class="button"><i class="fa-light fa-file"></i>&emsp14;PDF</a>
                            <a href="http://arxiv.org/abs/2409.01652" target="_blank" class="button"><i class="ai ai-arxiv"></i>&emsp14;arXiv</a>
                            <a href="https://youtu.be/2S8YhBdLdww" target="_blank" class="button"><i class="fa-light fa-film"></i>&emsp14;Video</a> -->
                            <!-- <a href="https://x.com/wenlong_huang/status/1829135436717142319" target="_blank" class="button"><i class="fa-brands fa-x-twitter"></i>&emsp14;tl;dr</a> -->
                            <!-- <a href="https://github.com/huangwl18/ReKep" target="_blank" class="button"><i class="fa-light fa-code"></i>&emsp14;Code</a> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-widescreen">
            <div class="hero-body">
                <div class="container">
                    <div class="columns is-vcentered  is-centered">
                        <video id="teaser" autoplay muted loop controls height="100%" width="100%">
                            <source src="media/videos/video_0924.mp4" type="video/mp4">
                        </video>
                        </br>
                    </div>
                    <br>
                    <h2 class="subtitle has-text-centered">
                        <!-- Objects are randomly placed within a region on the table. The robot must insert them into the correct receptacle in the Insertion Board, adjusting their orientation with the Fixture if necessary. -->
                    </h2>
                </div>
            </div>
        </div>

        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Generalizable long-horizon robotic assembly requires reasoning at multiple levels of
                            abstraction. End-to-end imitation learning (IL) has been proven a promising approach, but it
                            requires a large amount of demonstration data for training and often fails
                            to meet the high-precision requirement of assembly tasks. Reinforcement Learning (RL)
                            approaches have succeeded in high-precision assembly tasks, but suffer from sample
                            inefficiency and hence, are less competent at long-horizon
                            tasks. To address these challenges, we propose a hierarchical modular approach, named ARCH
                            (Adaptive Robotic Compositional Hierarchy), which enables long-horizon high-precision
                            assembly in contact-rich settings. ARCH employs
                            a hierarchical planning framework, including a low-level primitive library of continuously
                            parameterized skills and a high-level policy. The low-level primitive library includes
                            essential skills for assembly tasks, such as
                            grasping and inserting. These primitives consist of both RL and model-based controllers. The
                            high-level policy, learned via imitation learning from a handful of demonstrations, selects
                            the appropriate primitive skills and instantiates
                            them with continuous input parameters. We extensively evaluate our approach on a real robot
                            manipulation platform. We show that while trained on a single task, ARCH generalizes well to
                            unseen tasks and outperforms baseline
                            methods in terms of success rate and data efficiency.
                        </p>
                    </div>
                </div>
            </div>



            <!-- <hr class="rounded"> -->
            <!-- <div class="rows">
                <h2 class="title is-3">Walkthrough Video</h2>
                <div class="publication-video">
                    <iframe src="https://www.youtube.com/embed/2S8YhBdLdww" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                </div>
            </div> -->


            <hr class="rounded">
            <div class="rows">
                <h2 class="title is-3">Overview of ARCH</h2>
                <div class="column2">
                    <img src="media/figures/method.png" alt="method-imag" style="width:100%">
                </div>
                <div class="column2">
                    <img src="media/figures/overview.png" alt="method-image" style="width:100%">
                </div>
                <p class="content has-text-justified">
                    We propose a hierarchical framework for long-horizon robotic assembly. The high-level policy, which
                    takes as input object pose from pose estimation and robot proprioception, and outputs a categorical
                    distribution to select the best low-level primitive
                    as well as its continuous parameters, is obtained via imitation learning. The low-level policy
                    executes the selected primitive using either an RL-based or a motion-planned (MP) policy. We train
                    in simulation an RL policy for the contact-rich
                    portion of the task, e.g. insertion, based on force-torque feedback. We use the MP policies for
                    primitives that are in free space, e.g. move.
                </p>
            </div>

            <hr class="rounded">
            <div class="rows">
                <h2 class="title is-3">Generalization to Unseen Objects</h2>

                <!-- <p class="content has-text-justified">Since keypoints are tracked in real-time, the system can replan its actions in closed loop, both <i>within stages</i> and <i>across stages</i>. Here, the operator randomly perturbs the objects and the robot, but the robot can quickly react
                    to it. Note that after the robot tilts the teapot, if at this moment the cup is moved away from the robot, it would restore the teapot to be upright and attempt the pouring action again. Real-time solution is visualized on the right.</p>

                <div class="columns">
                    <div class="column has-text-centered">
                        <video id="dist1" controls autoplay loop muted width="100%">
        <source src="media/videos/pour-tea-dist.m4v" 
        type="video/mp4">
      </video>
                    </div>
                </div>
            </div>

            <hr class="rounded"> -->
                <!-- 
                <div class="rows">
                    <h2 class="title is-3">Bimanual Manipulation</h2>
                    <p class="content has-text-justified">
                        Bimanual tasks investigated in the paper. Select a task to see its video and solution visualization.
                    </p> -->
                <div class="columns">
                    <div class="column has-text-centered">
                        <div class="select is-rounded">
                            <select id="bimanual-video-menu" onchange="updateBimanual()">
                                <option value="circle" selected="selected">Circle (Unseen)</option>
                                <option value="doublesquare">Double Square (Unseen)</option>
                                <option value="oval">Oval (Unseen)</option>
                                <option value="3prong">3prong (Unseen)</option>
                                <option value="hexagon">Hexagon (Seen)</option>
                                <option value="star">Star (Unseen)</option>
                            </select>
                        </div>
                    </div>
                </div>

                <div class="columns">
                    <div class="column has-text-centered">
                        <p style="text-align:center;">
                            <video id="bimanual-video" width="100%" height="100%" controls autoplay loop muted>
                                <source src="media/videos/1_circle.mp4" type="video/mp4">
                            </video>
                        </p>
                    </div>
                </div>
            </div>

            <!-- <hr class="rounded"> -->

            <!-- <div class="rows">
                <h2 class="title is-3">In-The-Wild Manipulation</h2>
                <p class="content has-text-justified">
                    In-the-wild tasks investigated in the paper. Select a task to see its video and solution visualization.
                </p>
                <div class="columns">
                    <div class="column has-text-centered">
                        <div class="select is-rounded">
                            <select id="inthewild-video-menu" onchange="updateInTheWild()">
            <option value="stow-book" selected="selected">Stow Book</option>
          <option value="pour-tea">Pour Tea</option>
          <option value="tape-box">Tape Box</option>
          <option value="recycle-can">Recycle Can</option>
          </select>
                        </div>
                    </div>

                </div>

                <div class="columns">
                    <div class="column has-text-centered">
                        <p style="text-align:center;">
                            <video id="inthewild-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/pour-tea-dist.m4v" type="video/mp4">
          </video>
                        </p>
                    </div>
                </div>
            </div>



            <hr class="rounded"> -->

            <!-- <div class="rows">
                <h2 class="title is-3">Folding Clothes with Novel Strategies</h2>
                <p class="content has-text-justified">
                    The system can also generate novel strategies for the same task but under different scenarios. Specifically, we investigate whether the same system can fold different types of clothing items. Interestingly, we observe drastically different strategies
                    across the clothing categories, many of which align with how humans might fold each garment. Select a garment to see its folding strategy and its video. The coloring of the keypoints indicates the folding order, where red keypoints
                    are aligned first and the blue keypoints are aligned subsequently.
                </p>
                <div class="columns">
                    <div class="column has-text-centered">
                        <div class="select is-rounded">
                            <select id="clothes-video-menu" onchange="updateClothes()">
            <option value="sweater" selected="selected">Sweater</option>
          <option value="shirt">Shirt</option>
          <option value="hoodie">Hoodie</option>
          <option value="vest">Vest</option>
          <option value="dress">Dress</option>
          <option value="pants">Pants</option>
          <option value="shorts">Shorts</option>
          <option value="scarf">Scarf</option>
          </select>
                        </div>
                    </div>

                </div>

                <div class="columns">
                    <div class="column has-text-centered">
                        <p style="text-align:center;">
                            <img id="clothes-img" width="60%" src="media/fold-strategies/sweater.jpeg" class="method-image" />
                        </p>
                    </div>
                    <div class="column has-text-centered">
                        <p style="text-align:center;">
                            <video id="clothes-video" width="100%" height="100%" controls autoplay loop muted>
            <source src="media/videos/fold-sweater.mp4" type="video/mp4">
          </video>
                        </p>
                    </div>
                </div>
            </div> -->

            <!-- <hr class="rounded">
            <h2 class="title is-3">Acknowledgments</h2>
            <p>
                This work is partially supported by Stanford Institute for Human-Centered Artificial Intelligence, ONR MURI N00014-21-1-2801, and Schmidt Sciences. Ruohan Zhang is partially supported by Wu Tsai Human Performance Alliance Fellowship. The bimanual hardware
                is partially supported by Stanford TML. We would like to thank the anonymous reviewers, Albert Wu, Yifan Hou, Adrien Gaidon, Adam Harley, Christopher Agia, Edward Schmerling, Marco Pavone, Yunfan Jiang, Yixuan Wang, Sirui Chen, Chengshu
                Li, Josiah Wong, Wensi Ai, Weiyu Liu, Mengdi Xu, Yihe Tang, Chelsea Ye, Mijiu Mili, and the members of the Stanford Vision and Learning Lab for fruitful discussions, helps on experiments, and support.
            </p>
            <hr class="rounded"> -->
            <!-- <h2 class="title is-3">BibTeX</h2> -->
            <!-- <p class="bibtex"> -->
            <!-- @article{huang2024rekep, <br>
    &nbsp;&nbsp;title = {ReKep: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation}, <br>
    &nbsp;&nbsp;author = {Huang, Wenlong and Wang, Chen and Li, Yunzhu and Zhang, Ruohan and Fei-Fei, Li}, <br>
    &nbsp;&nbsp;journal = {arXiv preprint arXiv:2409.01652}, <br>
    &nbsp;&nbsp;year = {2024} <br>
    } -->
            <!-- </p> -->

    </section>
    </div>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a href="https://nerfies.github.io">Nerfies</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>


</body>

</html>